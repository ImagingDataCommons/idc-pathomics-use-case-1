{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-1.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m75"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "cptac_use_case.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PyITRKQCGpWY",
        "-OgrV_UTag7n",
        "_SS01aq3DnvS",
        "6RJclCq6Dw65"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWemgwQEODUQ"
      },
      "source": [
        "# only for development, can be removed later\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XZJ4whKODUM"
      },
      "source": [
        "# IDC Tutorial: Tissue classification in slide microscopy images\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ImagingDataCommons/idc-pathomics-use-case-1/blob/development/src/cptac_use_case.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use the [Imaging Data Commons (IDC)](https://portal.imaging.datacommons.cancer.gov/) for pathomics research:\n",
        "\n",
        "* how to select, access, and download cohorts from the IDC;\n",
        "* how to access and process slide microscopy images in DICOM format;\n",
        "* how to run an end-to-end analysis pipeline and visualize the final results exploiting the IDC platform.\n",
        "\n",
        "Pathomics refers to the computerized analysis of slide microscopy images. Besides radiology, slide microscopy is the second major imaging modality in the IDC. Slide microscopy images show thin sections of tissue samples (e.g., from a resected tumor) at microscopic resolution. They provide a unique glimpse into cellular architecture and function that is essential for diagnosing complex diseases like cancer. Computerized analysis makes the assessment of slide microscopy images more reproducible and less time consuming and it enables the extration of novel digital biomarkers from tissue images.\n",
        "\n",
        "This tutorial builds on the publication \"Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning\" ([Coudray et al. 2018](https://doi.org/10.1038/s41591-018-0177-5)), one of the most cited pathomics publications in recent years. A central use case from this publication is replicated on the CPTAC-LUAD and CPTAC-LSCC data sets in the IDC: the AI-based classification of lung tissue regions into <font color='green'>normal</font>, adenocarcinoma (<font color='red'>LUAD</font>), and squamous cell carcinoma (<font color='blue'>LSCC</font>) tissue. Your own tissue classification use cases can be solved in a similar manner.\n",
        "  \n",
        "To be quickly and freely accessible to everyone, this tutorial was deliberately kept simple and designed to be run in Google Colab. It highlights only a small part of what the IDC can offer in terms of data exploration and imaging analysis. More complex use cases can be implemented using GCP virtual machines. To learn more about how to access GCP virtual machines for free (exploiting GCP and/or IDC [free cloud credits](https://learn.canceridc.dev/introduction/requesting-gcp-cloud-credits)) and about the IDC platform, please visit the [IDC user guide](https://learn.canceridc.dev/).\n",
        "\n",
        "If you have any questions, bug reports, or feature requests please feel free to contact us at the [IDC discussion forum](https://discourse.canceridc.dev/)!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyITRKQCGpWY"
      },
      "source": [
        "## Customization\n",
        "\n",
        "Before we can begin, some individual adjustments must be made as described in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEBmNHEmDUlU"
      },
      "source": [
        "To access IDC resources, you have to authenticate with your **Google identity**. Follow the link generated by the code below and enter the displayed verification code to complete the Google authentication process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "EbKWd-q1ODUW"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRG25ZsVdOv-"
      },
      "source": [
        "Access to IDC resources must happen in the context of a **Google Cloud Platform project**. You can learn how to create your own project [here](https://www.youtube.com/watch?v=i08S0KJLnyw). Set `my_project_id` below to the ID of your GCP project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHV_UoHAODUX"
      },
      "source": [
        "my_project_id = 'idc-pathomics-000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD4pg2s1ODUT"
      },
      "source": [
        "This notebook is intended to be run using a **GPU**. In Google Colab, GPU usage can be enabled via `Edit > Notebooks Settings > Hardware accelerator`. Since Collab notebooks are assigned arbitrary GPUs (eg., NVIDIA K80, T4, P4 and P100), processing times can vary. The code below checks whether GPU usage has been enabled and which GPU type the Colab instance is equipped with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik2dHbpXODUU"
      },
      "source": [
        "gpu_list = !nvidia-smi --list-gpus\n",
        "has_gpu = False if 'failed' in gpu_list[0] else True\n",
        "print(has_gpu, gpu_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgrV_UTag7n"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJWt5C8CCNRU"
      },
      "source": [
        "This notebook relies on utility code for preprocessing, model training and evaluation, which is provided in an open-source Github repository. In Google Colab, the easiest way to retrieve subdirectories from GitHub is with the Apache Subversion tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1kAyZnZODUa"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y subversion \n",
        "!svn checkout https://github.com/ImagingDataCommons/idc-pathomics-use-case-1/branches/development/src # use trunk instead of branches/branch-name later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2PnoKQCd1K"
      },
      "source": [
        "Most of the Python packages required by this notebook are preinstalled in a Google Colab instance. In addition, we need to install the [OpenSlide](https://openslide.org/api/python/) library for accessing whole-slide images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXn9b0BfODUb"
      },
      "source": [
        "!sudo apt-get install --no-install-recommends -y python3-openslide\n",
        "!sudo pip3 install openslide-wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHDqvzCCzzj"
      },
      "source": [
        "Import the required Python modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWDMlEUXODUc"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/dist-packages') # otherwise Openslide cannot be loaded.\n",
        "import glob \n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "import warnings \n",
        "with warnings.catch_warnings(): # Hide python warnings to improve readability.\n",
        "    warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccy1m_azC8yh"
      },
      "source": [
        "Determine who and where we are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENz1CUE3ODUY"
      },
      "source": [
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print('Current directory :', curr_dir[-1])\n",
        "print('Hostname          :', curr_droid[-1])\n",
        "print('Username          :', curr_pilot[-1])\n",
        "!rm -rf sample_data # remove /content/sample_data directory which is included in a Google Colab instance by default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbKvSMirNKJa"
      },
      "source": [
        "Create directories for input and output data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFuEQ3tXNPDy"
      },
      "source": [
        "input_dir = '/content/idc_input/'\n",
        "output_dir = '/content/idc_output'\n",
        "# Use the following paths if using a Google Cloud VM instead of Google Colab\n",
        "#input_dir = '/home/jupyter/idc_input/'\n",
        "#output_dir = '/home/jupyter/idc_output/'\n",
        "\n",
        "if not os.path.exists(input_dir):\n",
        "    os.makedirs(input_dir)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGvZhkpyODUc"
      },
      "source": [
        "## Dataset selection and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM1g60Hx_Iij"
      },
      "source": [
        "IDC relies on the Google Cloud Platform (GCP) for storage and management of DICOM data. The data are contained in so-called [storage buckets](https://cloud.google.com/storage/docs/key-terms#buckets), from which they can be retrieved on a requester pays basis. Currently, all pathology whole-slide images (WSI) are located in the `idc-open` bucket.\n",
        "\n",
        "Metadata for the DICOM files—including standard DICOM tags, but also non-DICOM metadata—are stored in the BigQuery table `dicom_all`. The IDC Documentation gives further information on [data organization](https://learn.canceridc.dev/data/organization-of-data) and [code examples](https://learn.canceridc.dev/cookbook/bigquery) on how to query the table. The easiest way to access BigQuery tables from a Jupyter notebook is to use [BigQuery cell magic](https://cloud.google.com/bigquery/docs/visualize-jupyter#querying-and-visualizing-bigquery-data) using the `%%bigquery` command. \n",
        "\n",
        "The following statement loads relevant metadata of all slide images from the CPTAC-LUAD and CPTAC-LSCC datasets into a pandas data frame called `slides_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSIKW0VJODUe"
      },
      "source": [
        "%%bigquery slides_df --project=$my_project_id \n",
        "\n",
        "SELECT\n",
        "    ContainerIdentifier AS slide_id,\n",
        "    PatientID AS patient_id,\n",
        "    ClinicalTrialProtocolID AS dataset,\n",
        "    TotalPixelMatrixColumns AS width,\n",
        "    TotalPixelMatrixRows AS height,\n",
        "    StudyInstanceUID AS idc_viewer_id,        \n",
        "    gcs_url, -- URL of the Google Cloud storage bucket\n",
        "    CAST(SharedFunctionalGroupsSequence[OFFSET(0)].\n",
        "          PixelMeasuresSequence[OFFSET(0)].\n",
        "          PixelSpacing[OFFSET(0)] AS FLOAT64) AS pixel_spacing,\n",
        "    -- rename TransferSyntaxUIDs for readability\n",
        "    CASE TransferSyntaxUID\n",
        "        WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'\n",
        "        WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'\n",
        "        ELSE 'other'\n",
        "    END AS compression\n",
        "FROM idc-dev-etl.idc_v3.dicom_all\n",
        "WHERE\n",
        "  NOT (ContainerIdentifier IS NULL)\n",
        "  AND (ClinicalTrialProtocolID = \"CPTAC-LUAD\" OR ClinicalTrialProtocolID = \"CPTAC-LSCC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk8XYRkhAHu8"
      },
      "source": [
        "We reduce the obtained data frame to the images we actually want to use for the analysis. Those are required to be digitized at 20x magnification (corresponding to a pixel spacing between 0.00025 and 0.00051 mm) and compressed in JPEG format (to be readable by the OpenSlide libray). Also, we consider only every 5th slide to make the dataset small enough to be manageable in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGhIxoWh5bU0"
      },
      "source": [
        "slides_df.query('pixel_spacing > 0.00025 & pixel_spacing < 0.00051 & compression==\"jpeg\"', inplace=True)\n",
        "slides_df = slides_df.iloc[::5, :] # select every 5th row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUDmRLQaODUf"
      },
      "source": [
        "The tissue type of the slides (tumor or normal) is not yet included in the `dicom_all` table. This information has to be supplemented from a separate CSV file provided by the TCIA. The cancer subtype (LSCC or LUAD) can in principle be inferred from the dataset name, but for clarity we also use the information in the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g84bWWjBODUg"
      },
      "source": [
        "type_df = pd.read_csv('src/tissue_type_data_TCIA.csv')[['Slide_ID', 'Specimen_Type', 'Tumor']]\n",
        "# harmonize column names and labels\n",
        "type_df.rename(columns={'Slide_ID': 'slide_id', 'Specimen_Type': 'tissue_type', 'Tumor': 'cancer_subtype'}, inplace=True)\n",
        "type_df.replace({'tissue_type': {'normal_tissue': 'normal', 'tumor_tissue': 'tumor'}}, inplace=True)\n",
        "type_df.replace({'cancer_subtype': {'LSCC': 'lscc', 'LUAD': 'luad'}}, inplace=True)\n",
        "slides_df = pd.merge(slides_df, type_df, how='inner', on='slide_id', sort=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CqUnbMUODUg"
      },
      "source": [
        "With standard [pandas](https://pandas.pydata.org/) functionality, we can easily validate and summarize the compiled metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IhrfoqyODUh"
      },
      "source": [
        "# assert uniqueness of slide_id values\n",
        "assert(slides_df.slide_id.is_unique)\n",
        "\n",
        "# assert validity of class labels\n",
        "assert set(slides_df.tissue_type.unique()) == set(['normal', 'tumor'])\n",
        "assert set(slides_df.cancer_subtype.unique()) == set(['luad', 'lscc'])\n",
        "\n",
        "display(slides_df)\n",
        "print('Total number of slides: ', len(slides_df))\n",
        "nr_slides = slides_df.groupby('cancer_subtype').size()\n",
        "nr_patients = slides_df.drop_duplicates('patient_id').groupby('cancer_subtype').size()\n",
        "print('--> %d slides from %d LUAD patients' % (nr_slides['luad'], nr_patients['luad']))\n",
        "print('--> %d slides from %d LSCC patients' % (nr_slides['lscc'], nr_patients['lscc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnDVVjlPODUh"
      },
      "source": [
        "Using standard [matplotlib](https://matplotlib.org/) functionality, we can easily visualize some aspects of interest. The following code produces two histograms. The left graph shows the frequencies of numbers of slides per per patient, while the right graph shows the proportions of slides derived from healthy or tumor tissue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZR93DngODUh"
      },
      "source": [
        "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# number of slides per patient \n",
        "slides_per_patient = slides_df.groupby(['patient_id']).size()\n",
        "plot1 = sns.histplot(data=slides_per_patient, discrete=True, ax=ax1, shrink=0.9, color=['C7'])\n",
        "ax1.update({'xlabel': 'Number of slides', 'ylabel': 'Number of patients'})\n",
        "\n",
        "# distribution of tissue types\n",
        "plot2 = sns.histplot(data=slides_df, x='cancer_subtype', hue='tissue_type', multiple='stack', palette = ['C1', 'C2'], ax=ax2, shrink=0.7)\n",
        "ax2.update ({'xlabel': 'Cancer subtype', 'ylabel': 'Number of slides'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM5lb-qLODUh"
      },
      "source": [
        "Any slide can also be easily viewed and explored in detail using the IDC viewer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eQc54DODUi"
      },
      "source": [
        "def get_idc_viewer_url(study_UID):\n",
        "    return \"https://viewer.imaging.datacommons.cancer.gov/slim/studies/\" + study_UID\n",
        "\n",
        "print(get_idc_viewer_url(slides_df['idc_viewer_id'].iloc[0]))\n",
        "print(get_idc_viewer_url(slides_df['idc_viewer_id'].iloc[100]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRA-t7_v8Yv3"
      },
      "source": [
        "Finally, we save the information the CSV file `slides_metadata.csv` to be used later for splitting into training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF59Er-b8itA"
      },
      "source": [
        "slides_df.to_csv(os.path.join(input_dir, 'slides_metadata.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SS01aq3DnvS"
      },
      "source": [
        "## Pathomics experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgjuwjmXODUi"
      },
      "source": [
        "The following pathomics experiment consists of three main steps: \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `1.` **Preprocessing**: \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `a.` Tiling of slides and filtering out of background tiles \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `b.` Separation into training, validation and test dataset \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `2.` **Training**: Per-tile training of the model \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `3.` **Evaluation**: Per-tile and per-slide evaluation of the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbOrYn_PODUi"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "**Tile generation**: First, the WSI are downloaded with `gsutil` and the respective \"gcs_url\"s. Then each slide is tiled into non-overlapping 128x128 pixel windows at a magnification of 5x using the python openslide library. As this code is supposed to serve as a demo requiring a minimum amount of runtime, we reduce each datasets size by generating only one in `twenty` tiles. \n",
        "\n",
        "**Generation of datasets**: Next, the tiles are distributed among / sorted into training (70%), test (15%) and validation (15%) datasets, ensuring that tiles associated with a particular patient are not separated, but assigned as a whole to one of these sets. For this purpose, a file (*patient_metadata.csv*) is created that reports the number of tiles created for each patient and is used to generate Figure 3 below. \\\n",
        "Additionally, we update *slides_metadata.csv* by adding the information whether a slide has been assigned to the training, validation or testset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sqJ3DDBODUj"
      },
      "source": [
        "from src.data.tile_generation_cptac import generate_tiles\n",
        "\n",
        "slides_dir = os.path.join(input_dir, 'cptac_slides')\n",
        "tiles_dir = os.path.join(input_dir, 'cptac_tiles')\n",
        "#os.mkdir(slides_dir)\n",
        "#os.mkdir(tiles_dir)\n",
        "\n",
        "#generate_tiles(slides_dir, slides_metadata_path, tiles_dir, save_every_xth_tile=20, google_cloud_project_id=my_project_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRHbIeWSODUj"
      },
      "source": [
        "from src.data.tile_sorting_cptac import sort_tiles\n",
        "\n",
        "sort_tiles(tiles_dir, slides_metadata_path, output_folder=input_dir)\n",
        "slides_metadata = pd.read_csv(slides_metadata_path) # reload updated slides_metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftBwYUG8ODUj"
      },
      "source": [
        "# Visualize some tiles from the training dataset\n",
        "training_data_list = pd.read_csv(os.path.join(input_dir, 'train_norm_luad_lscc.csv'))\n",
        "labels = {0:'normal', 1:'LUAD', 2:'LUSC'}\n",
        "fig2, axes = plt.subplots(1,6, figsize=(18,3))\n",
        "fig2.suptitle('Figure 2: Tiles from the training data')\n",
        "for i, idx in enumerate([125000*x for x in range(6)]): \n",
        "    axes[i].imshow(plt.imread(os.path.join(input_dir, training_data_list['path'][idx])))\n",
        "    axes[i].set_title(labels[training_data_list['reference_value'][idx]])\n",
        "    axes[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0q_tCWbODUk"
      },
      "source": [
        "# How many tiles per patient (due to different size of slides)?\n",
        "patient_metadata = pd.read_csv(os.path.join(input_dir, 'patient_metadata.csv'))\n",
        "print('Total number of tiles: {:,}'.format(patient_metadata['nr_tiles_total'].sum()))\n",
        "\n",
        "fig3 = sns.histplot(data=patient_metadata, x='nr_tiles_total', hue='cancer_subtype', multiple='stack', palette=['r', 'b'])\n",
        "fig3.set_title('Figure 3: Number of tiles per patient')\n",
        "fig3.set_xlabel('Number of tiles')\n",
        "fig3.set_ylabel('Number of patients')\n",
        "legend = fig3.get_legend()\n",
        "legend.set_title('Cancer subtype')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vXZO89ODUl"
      },
      "source": [
        "# Only for execution in VM --> remove later!\n",
        "# Create smaller training / validation and test sets (1/10 of the tiles)\n",
        "!sed -n '1p;0~40p' \"$input_dir/train_norm_luad_lscc.csv\" > \"$input_dir/train_norm_luad_lscc_40.csv\"\n",
        "!sed -n '1p;0~40p' \"$input_dir/valid_norm_luad_lscc.csv\" > \"$input_dir/valid_norm_luad_lscc_40.csv\"\n",
        "!sed -n '1p;0~40p' \"$input_dir/test_norm_luad_lscc.csv\" > \"$input_dir/test_norm_luad_lscc_40.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOQjGht5ODUl"
      },
      "source": [
        "### Training\n",
        "\n",
        "**Network training**: Following Coudray *et al.*, we use the InceptionV3 network architecture as implemented by Keras and shown in the figure below. The loss function is here defined as categorical cross entropy between the true labels and the network's predictions. RMSprop optimizer is utilized for learning with a learning rate of 0.1, weight decay of 0.9, momentum of 0.9 and epsilon of 1.0. Model checkpoints are automatically saved at each epoch in which the validation loss improves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbvT2mBXODUn"
      },
      "source": [
        "![inceptionV3.png](attachment:inceptionV3.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfRwefypODUn"
      },
      "source": [
        "from src.data.data_set import Dataset\n",
        "from src.model.inceptionv3_model import InceptionModel\n",
        "\n",
        "output_dir_experiment = os.path.join(output_dir, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
        "os.mkdir(output_dir_experiment)\n",
        "\n",
        "# Load datasets\n",
        "dataset_train = Dataset(os.path.join(input_dir, 'train_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "dataset_valid = Dataset(os.path.join(input_dir, 'valid_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "\n",
        "# Model set-up and training\n",
        "model = InceptionModel(num_classes=3, input_shape=(128,128,3), learning_rate=0.1)\n",
        "model.train(dataset_train, batch_size=512, epochs=10, output_path=output_dir_experiment, validation_dataset=dataset_valid) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sem3CkicODUo"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "The final model is selected as the one with the best (minimal) loss on the validation dataset. The performance of this model is evaluated quantitatively by means of receiver operating characteristic (ROC) curves and observed qualitatively using heatmap visualization. First, the model makes predictions for the entire test set and stores them in the experiment's output directory to make subsequent evaluation easily reproducible. \n",
        "\n",
        "**ROC analysis** for each of the three classes (normal, LUAD, LSCC) is performed on **tile-level** (tile-based ROC) and on **slide-level** (slide-based ROC). For slide-level analysis the probabilities for each slide were aggregated either by averaging the probabilities of the corresponding tiles, or by counting the percentage of tiles positively classified. Respective Area under the ROC curve (AUC) values are reported in the table below and the slide-based ROC curves are visualized in Figure 4. In addition to the ROC curves for each class, a micro-average ROC was computed and visualized. \n",
        "\n",
        "**Heatmaps** for visually inspecting the network's predictions are generated for some of the slides. The color of each tile corresponds to the class assigned by our model, with the hue indicating how confident the network is in this classification whereby a darker hue corresponds to higher confidence. The respective orginal WSI is shown as thumbnail image next to the heatmap. The generation of a thumbnail image takes ~1min, thus we limit ourselves to showing heatmaps for only 4 random slides from the testset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atrZ9kPbODUo"
      },
      "source": [
        "# Load the best model \n",
        "checkpoints = glob.glob(os.path.join(output_dir_experiment, 'checkpoint*/'))\n",
        "checkpoints.sort()\n",
        "best_model_dir = checkpoints[-1] # last checkpoint is the one with the best validation loss \n",
        "\n",
        "#from model.inceptionv3_model import InceptionModel\n",
        "#from data.data_set import Dataset\n",
        "#output_dir_experiment='/home/jupyter/idc_output/20210818_161953/'\n",
        "#best_model_dir = '/home/jupyter/idc_output/20210818_161953/checkpoint_008'\n",
        "\n",
        "best_model = InceptionModel.load(best_model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxm0hfOuODUp"
      },
      "source": [
        "from src.evaluation.predictions import Predictions\n",
        "\n",
        "test_dataset = Dataset(os.path.join(input_dir, 'test_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "predictions = Predictions(best_model, test_dataset) \n",
        "predictions.save(os.path.join(output_dir_experiment, 'predictions_testset_40.json'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQBE9XSODUp"
      },
      "source": [
        "from src.evaluation.roc import ROCAnalysis\n",
        "\n",
        "roc_analysis = ROCAnalysis(predictions)\n",
        "roc_analysis.print_and_save_tabluar_results(os.path.join(output_dir_experiment, 'results_table_40.html'))\n",
        "roc_analysis.plot_and_save(output_dir_experiment) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aer0qDMhODUq"
      },
      "source": [
        "%%capture --no-display\n",
        "from src.data.utils import get_random_testset_slide_ids, get_thumbnail, get_slide_tissue_type\n",
        "from src.evaluation.heatmap import get_heatmap, plot_colormap_legend\n",
        "from src.evaluation.predictions import Predictions\n",
        "\n",
        "# Select slides and download thumbnails \n",
        "heatmap_slide_ids = get_random_testset_slide_ids(slides_metadata)\n",
        "#get_thumbnail(heatmap_slide_ids, slides_metadata_path, slides_dir, google_cloud_project_id=my_project_id) # NOTE: lasts ~1min per slide\n",
        "\n",
        "# Generate figure \n",
        "plot_colormap_legend()\n",
        "fig5, axes = plt.subplots(4, 2, figsize=(10, 9))\n",
        "fig5.suptitle('Figure 5: Heatmap visualization')\n",
        "for i in range(4):\n",
        "    slide_id = 'C3L-00913-22'#heatmap_slide_ids[i]\n",
        "    slide = plt.imread(os.path.join(slides_dir, slide_id + '.png'))\n",
        "    true_tissue_type = get_slide_tissue_type(slide_id, slides_metadata)\n",
        "    axes[i,0].imshow(get_heatmap(predictions, slide_id))\n",
        "    axes[i,1].imshow(slide1)\n",
        "    axes[i,1].set_title(true_tissue_type.upper())\n",
        "    axes[i,0].axis('off')\n",
        "    axes[i,1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RJclCq6Dw65"
      },
      "source": [
        "## Notes for improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU42L0ulODUr"
      },
      "source": [
        "### TODOs / Notes for myself \n",
        "- **Experienced in Colab that training progress is not printed and it takes quite long..what to do about that, if it appears more often?**\n",
        "    - AttributeError: 'NoneType' object has no attribute 'shape' --> documented in 03_pitfalls, maybe discuss with Andre\n",
        "- Check whether we have to change back model(patch_to_predict) to model.predict(patch_to_predict) when using TF/Keras 2.6.0\n",
        "- Check in general, that everything installed comes with a specific version\n",
        "- Add a comment about how long tile generation lasts for the user to know beforehand\n",
        "- Maybe verify successfull installations in notebook to catch errors there? \n",
        "- Get that \"Open in colab\" batch centered\n",
        "- sys.path.append('/usr/local/lib/python3.7/dist-packages') # otherwise Openslide cannot be loaded. Maybe find a better solution?!\n",
        "- Remove creation of smaller datasets from large files for everything on Google Colab \n",
        "- Heatmap add predicted class --> then we would have to decide on the threshold to use. \n",
        "- Adapt stepsize = len(training_data_list) // 10 when visualizing training tiles to make nice onces appear. \n",
        "- Remove try-except statement when opening slides --> we expect, that it works for all of them?!\n",
        "\n",
        "### What to polish\n",
        "--> Walk through all files to find futher stuff \n",
        "- Remove make_single_prediction?\n",
        "- Remove class_weights parameter completely from the code?\n",
        "- Check that TCGA-related stuff is removed or generalized e.g. in data_point \n",
        "- Make code nicer in roc.py when not showing confidence intervals \n",
        "\n",
        "### Final check\n",
        "- Move to IDC-Examples: adapt svn checkout, Github links in Environment setup text and in Colab Button\n",
        "- Check that all requirements are installed\n",
        "- Check that all warnings are suppressed/captured that we want to be captured\n",
        "- Adapt path to correct idc-Bucket and Slim-viewer\n",
        "- Check that everything which is only for execution in the VM / development purposes is removed.\n",
        "- Walk through explanatory text and ensure everything is correct! \n",
        "    - Generation of every x-th tile: Check that x is set correctly and matches what stands in the text\n",
        "    - Adapt text about how many slides there are etc. \n",
        "- Make sure, that notebook is saved without output or rather with correct and nice looking output \n",
        "- Check if all images (e.g. InceptionV3 or plotted figures) are displayed nicely in Colab\n",
        "\n",
        "### Other notes: \n",
        "- claim GPU only when we are using it, i.e. for training! --> GPU has to be included at the very beginning, other stuff stored is removed otherwise (even Google authentication and variables) \n",
        "\n",
        "## To run notebook on VM\n",
        "- remove Google Colab authentification\n",
        "- remove GPU activation\n",
        "- remove Github/svn checkout\n",
        "- change input/output directory paths \n",
        "- Add again creation of smaller datasets from large files (and set save_every_xth_tile=1, if tiles not already downloaded)\n",
        "\n",
        "## Open questions\n",
        "- Why do we have so much more tiles than with TCGA data? \n",
        "\n",
        "### Answered:\n",
        "- When will CPTAC data go to official IDC and in which bucket?\n",
        "    - very soon and into the idc-open bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rorARTX4ODUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}