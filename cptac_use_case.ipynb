{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-1.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m75"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "cptac_use_case.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-OgrV_UTag7n",
        "PyITRKQCGpWY",
        "ZGvZhkpyODUc",
        "AXo6KctwODUd",
        "_SS01aq3DnvS",
        "GgjuwjmXODUi",
        "6RJclCq6Dw65"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWemgwQEODUQ"
      },
      "source": [
        "# only for development, can be removed later\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XZJ4whKODUM"
      },
      "source": [
        "# IDC Tutorial: Tissue classification in slide microscopy images\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ImagingDataCommons/idc-pathomics-use-case-1/blob/development/src/cptac_use_case.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use the [Imaging Data Commons (IDC)](https://portal.imaging.datacommons.cancer.gov/) and [Google Cloud Computing (GCP)](https://cloud.google.com) platforms for pathomics research:\n",
        "\n",
        "* how to select, access, and download cohorts from the IDC;\n",
        "* how to access and process slide microscopy images in DICOM format;\n",
        "* how to run an end-to-end analysis pipeline and visualize the final results exploiting the IDC platform.\n",
        "\n",
        "Pathomics refers to the computerized analysis of slide microscopy images. Besides radiology, slide microscopy is the second major imaging modality in the IDC. Slide microscopy images show thin sections of tissue samples (e.g., from a resected tumor) at microscopic resolution. They provide a unique glimpse into cellular architecture and function that is essential for diagnosing complex diseases like cancer. Computerized analysis makes the assessment of slide microscopy images more reproducible and less time consuming and it enables the extration of novel digital biomarkers from tissue images.\n",
        "\n",
        "This tutorial builds on the publication \"Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning\" ([Coudray et al. 2018](https://doi.org/10.1038/s41591-018-0177-5)), one of the most cited pathomics publications in recent years. A central use case from this publication is replicated on the CPTAC-LUAD and CPTAC-LSCC data sets in the IDC: the AI-based classification of lung tissue regions into <font color='green'>normal</font>, adenocarcinoma (<font color='red'>LUAD</font>), and squamous cell carcinoma (<font color='blue'>LSCC</font>) tissue. Your own tissue classification use cases can be solved in a similar manner.\n",
        "  \n",
        "To be quickly and freely accessible to everyone, this tutorial was deliberately kept simple and designed to be runnable in Google Colab. It highlights only a small part of what the IDC can offer in terms of data exploration and imaging analysis. More complex use cases can be implemented using GCP virtual machines. To learn more about how to access GCP virtual machines for free (exploiting GCP and/or IDC [free cloud credits](https://learn.canceridc.dev/introduction/requesting-gcp-cloud-credits)) and about the IDC platform, please visit the [IDC user guide](https://learn.canceridc.dev/).\n",
        "\n",
        "If you have any questions, bug reports, or feature requests please feel free to contact us at the [IDC discussion forum](https://discourse.canceridc.dev/)!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgrV_UTag7n"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJWt5C8CCNRU"
      },
      "source": [
        "This notebook relies on utility code for preprocessing, model training and evaluation, which is provided in an open-source Github repository. In Google Colab, the easiest way to retrieve subdirectories from GitHub is with the Apache Subversion tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1kAyZnZODUa"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y subversion \n",
        "!svn checkout https://github.com/ImagingDataCommons/idc-pathomics-use-case-1/branches/development/src # use trunk instead of branches/branch-name later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE2PnoKQCd1K"
      },
      "source": [
        "Most of the Python packages required by this notebook are preinstalled in a Google Colab instance. In addition, we need to install the [OpenSlide](https://openslide.org/api/python/) library for accessing whole-slide images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXn9b0BfODUb"
      },
      "source": [
        "!sudo apt-get install --no-install-recommends -y python3-openslide\n",
        "!sudo pip3 install openslide-wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHDqvzCCzzj"
      },
      "source": [
        "Import the required Python modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWDMlEUXODUc"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/dist-packages') # otherwise Openslide cannot be loaded.\n",
        "import glob \n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "import warnings \n",
        "with warnings.catch_warnings(): # Hide python warnings to improve readability.\n",
        "    warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccy1m_azC8yh"
      },
      "source": [
        "Determine who and where we are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENz1CUE3ODUY"
      },
      "source": [
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print('Current directory :', curr_dir[-1])\n",
        "print('Hostname          :', curr_droid[-1])\n",
        "print('Username          :', curr_pilot[-1])\n",
        "!rm -rf sample_data # remove /content/sample_data directory which is included in a Google Colab instance by default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyITRKQCGpWY"
      },
      "source": [
        "## Custom settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEBmNHEmDUlU"
      },
      "source": [
        "To access IDC resources, you have to authenticate with your **Google identity**. Follow the link generated by the code below and enter the displayed verification code to complete the Google authentication process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "EbKWd-q1ODUW"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRG25ZsVdOv-"
      },
      "source": [
        "Access to IDC resources must happen in the context of a **Google Cloud Platform project**. You can learn how to create your own project [here](https://www.youtube.com/watch?v=i08S0KJLnyw). Set `my_project_id` below to the ID of your GCP project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHV_UoHAODUX"
      },
      "source": [
        "my_project_id = 'idc-pathomics-000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD4pg2s1ODUT"
      },
      "source": [
        "This notebook is intended to be run using a **GPU**. In Google Colab, GPU usage can be enabled via `Edit > Notebooks Settings > Hardware accelerator`. Since Collab notebooks are assigned arbitrary GPUs (eg., NVIDIA K80, T4, P4 and P100), processing times can vary. The code below checks whether GPU usage has been enabled and which GPU type the Colab instance is equipped with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik2dHbpXODUU"
      },
      "source": [
        "gpu_list = !nvidia-smi --list-gpus\n",
        "has_gpu = False if 'failed' in gpu_list[0] else True\n",
        "print(has_gpu, gpu_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGvZhkpyODUc"
      },
      "source": [
        "## Dataset selection and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXo6KctwODUd"
      },
      "source": [
        "### Dataset selection using BigQuery\n",
        "\n",
        "IDC relies on the Google Cloud Platform (GCP) for storage and management of DICOM data. The data are contained in so-called [storage buckets](https://cloud.google.com/storage/docs/key-terms#buckets), from which they are available for the user on a requester pays basis. Currently, all pathology whole-slide images (WSI) are located in the `idc-open` bucket.\n",
        "**Metadata** for the DICOM files - including standard DICOM tags, but also non-DICOM metadata - are stored in BigQuery tables from which they can easily be accessed using standard SQL queries. This gives users the opportunity to explore available data by examining metadata without the need of downloading the DICOM data in the first place. Additional information on the organization of data can be found in the [IDC Documentation](https://learn.canceridc.dev/data/organization-of-data). \n",
        "\n",
        "The easiest way to acess BigQuery tables from within a Jupyter notebook is to use the [BigQuery cell magic](https://cloud.google.com/bigquery/docs/visualize-jupyter#querying-and-visualizing-bigquery-data) provided by the BigQuery client library for Python. \n",
        "Using the `%%bigquery` command as shown below, the results of the following SQL query are stored as pandas data frame `cohort_df`.   \n",
        "Further information on how to use BigQuery within the IDC, on the attributes available, and code examples can be found [here](https://learn.canceridc.dev/cookbook/bigquery)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezJRAUZcODUe"
      },
      "source": [
        "# First of all, define and create directories for input and output data \n",
        "\n",
        "# in VM\n",
        "#input_dir = '/home/jupyter/idc_input/'\n",
        "#output_dir = '/home/jupyter/idc_output/'\n",
        "\n",
        "input_dir = '/content/idc_input/'\n",
        "output_dir = '/content/idc_output'\n",
        "os.mkdir(input_dir)\n",
        "os.mkdir(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSIKW0VJODUe"
      },
      "source": [
        "%%bigquery cohort_df --project=$my_project_id \n",
        "\n",
        "WITH slide_images AS (\n",
        "    SELECT\n",
        "        ContainerIdentifier AS slide_id,\n",
        "        PatientID AS patient_id,\n",
        "        ClinicalTrialProtocolID AS dataset,\n",
        "        TotalPixelMatrixColumns AS width,\n",
        "        TotalPixelMatrixRows AS height,\n",
        "        StudyInstanceUID AS idc_viewer_id,\n",
        "        gcs_url,\n",
        "        CAST(SharedFunctionalGroupsSequence[OFFSET(0)].\n",
        "             PixelMeasuresSequence[OFFSET(0)].\n",
        "             PixelSpacing[OFFSET(0)] AS FLOAT64) AS pixel_spacing,\n",
        "        CASE TransferSyntaxUID\n",
        "            WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'\n",
        "            WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'\n",
        "            ELSE 'other'\n",
        "        END AS compression\n",
        "    FROM idc-dev-etl.idc_v3.dicom_all\n",
        "    WHERE NOT (ContainerIdentifier IS NULL)\n",
        ")\n",
        "SELECT *\n",
        "FROM slide_images\n",
        "WHERE\n",
        "    (compression = \"jpeg\")\n",
        "    AND (dataset = \"CPTAC-LUAD\" OR dataset = \"CPTAC-LSCC\")\n",
        "    -- pixel spacing between 0.00025 and 0.00051 mm corresponds to 20x magnification\n",
        "    AND (pixel_spacing > 0.00025) AND (pixel_spacing < 0.00051)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUDmRLQaODUf"
      },
      "source": [
        "Since up to now the information about the slide's tissue type (tumor tissue or normal tissue) is missing in the BQ table, we have to extract these from the attached TCIA metadata file and add them to our cohort's metadata. We remove slides that have no tissue type information available from our cohort. The final cohort' metadata are saved as *slides_metadata.csv*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g84bWWjBODUg"
      },
      "source": [
        "from src.data.utils import attach_tissue_type_information\n",
        "\n",
        "tissue_type_data = pd.read_csv('src/tissue_type_data_TCIA.csv')\n",
        "slides_metadata = attach_tissue_type_information(cohort_df, tissue_type_data)\n",
        "slides_metadata_path = os.path.join(input_dir, 'slides_metadata.csv')\n",
        "slides_metadata.to_csv(slides_metadata_path, index=False) # Save slides metadata as csv (used later for distribution of slides into train/validation/test set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CqUnbMUODUg"
      },
      "source": [
        "### Dataset exploration using Python functionality\n",
        "\n",
        "Using pandas and matplotlib functionality we can easily explore the selected cohort's metadata and visualize some aspects of interest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IhrfoqyODUh"
      },
      "source": [
        "display(slides_metadata.head())\n",
        "\n",
        "print('Total number of slides in this cohort: ', len(slides_metadata))\n",
        "nr_slides_from_luad_patients = slides_metadata.groupby('cancer_subtype').size()['luad']  \n",
        "nr_slides_from_lscc_patients = slides_metadata.groupby('cancer_subtype').size()['lscc']\n",
        "nr_luad_patients = slides_metadata.drop_duplicates('patient_id').groupby('cancer_subtype').size()['luad']\n",
        "nr_lscc_patients = slides_metadata.drop_duplicates('patient_id').groupby('cancer_subtype').size()['lscc']\n",
        "print('--> %d slides from %d LUAD patients' %(nr_slides_from_luad_patients, nr_luad_patients))\n",
        "print('--> %d slides from %d LSCC patients' %(nr_slides_from_lscc_patients, nr_lscc_patients))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnDVVjlPODUh"
      },
      "source": [
        "Since we have ~1000 slides from ~200 patients for each cancer subtype, there are on average 5 WSI per patient. Next, we explore the distributions of slides as histogram.\n",
        "\n",
        "**Important:** In both the CPTAC-LUAD and CPTAC-LSCC datasets, not only tumor tissue but also healthy (\"normal\") tissue samples were collected from each patient. The right plot reveals that about one third of the slides were derived from healthy tissue, while the other two thirds were derived from cancerous tissue. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZR93DngODUh"
      },
      "source": [
        "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "fig1.suptitle('Figure 1: Number of slides per patient / Distribution of tissue types')\n",
        "\n",
        "# How many slides per patient? \n",
        "slides_per_patient = slides_metadata.groupby(['patient_id', 'cancer_subtype']).size()\n",
        "plot1 = sns.histplot(data=slides_per_patient, discrete=True, ax=ax1, shrink=0.9, color=['C7'])\n",
        "ax1.set_xlabel('Number of slides')\n",
        "ax1.set_ylabel('Number of patients')\n",
        "\n",
        "# How is the relation between slides from healthy / cancerous tissue per cancer subtype?\n",
        "plot2 = sns.histplot(data=slides_metadata, x='cancer_subtype', hue='tissue_type', multiple='stack', palette = ['C1', 'C2'], ax=ax2, shrink=0.7)\n",
        "ax2.set_xlabel('Cancer subtype')\n",
        "ax2.set_ylabel('Number of slides')\n",
        "legend = plot2.get_legend()\n",
        "legend.set_title('Tissue type')\n",
        "legend.set_bbox_to_anchor((1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM5lb-qLODUh"
      },
      "source": [
        "Any slide can also be easily investigated in detail using the IDC viewer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eQc54DODUi"
      },
      "source": [
        "def get_idc_viewer_url(study_UID):\n",
        "    return \"https://viewer.imaging.datacommons.cancer.gov/slim/studies/\" + study_UID\n",
        "\n",
        "print(get_idc_viewer_url(slides_metadata['idc_viewer_id'].iloc[2000]))\n",
        "print(get_idc_viewer_url(slides_metadata['idc_viewer_id'].iloc[2000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SS01aq3DnvS"
      },
      "source": [
        "## Pathomics experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgjuwjmXODUi"
      },
      "source": [
        "The following pathomics experiment consists of three main steps: \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `1.` **Preprocessing**: \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `a.` Tiling of slides and filtering out of background tiles \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `b.` Separation into training, validation and test dataset \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `2.` **Training**: Per-tile training of the model \\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; `3.` **Evaluation**: Per-tile and per-slide evaluation of the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbOrYn_PODUi"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "**Tile generation**: First, the WSI are downloaded with `gsutil` and the respective \"gcs_url\"s. Then each slide is tiled into non-overlapping 128x128 pixel windows at a magnification of 5x using the python openslide library. As this code is supposed to serve as a demo requiring a minimum amount of runtime, we reduce each datasets size by generating only one in `twenty` tiles. \n",
        "\n",
        "**Generation of datasets**: Next, the tiles are distributed among / sorted into training (70%), test (15%) and validation (15%) datasets, ensuring that tiles associated with a particular patient are not separated, but assigned as a whole to one of these sets. For this purpose, a file (*patient_metadata.csv*) is created that reports the number of tiles created for each patient and is used to generate Figure 3 below. \\\n",
        "Additionally, we update *slides_metadata.csv* by adding the information whether a slide has been assigned to the training, validation or testset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sqJ3DDBODUj"
      },
      "source": [
        "from src.data.tile_generation_cptac import generate_tiles\n",
        "\n",
        "slides_dir = os.path.join(input_dir, 'cptac_slides')\n",
        "tiles_dir = os.path.join(input_dir, 'cptac_tiles')\n",
        "#os.mkdir(slides_dir)\n",
        "#os.mkdir(tiles_dir)\n",
        "\n",
        "#generate_tiles(slides_dir, slides_metadata_path, tiles_dir, save_every_xth_tile=20, google_cloud_project_id=my_project_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRHbIeWSODUj"
      },
      "source": [
        "from src.data.tile_sorting_cptac import sort_tiles\n",
        "\n",
        "sort_tiles(tiles_dir, slides_metadata_path, output_folder=input_dir)\n",
        "slides_metadata = pd.read_csv(slides_metadata_path) # reload updated slides_metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftBwYUG8ODUj"
      },
      "source": [
        "# Visualize some tiles from the training dataset\n",
        "training_data_list = pd.read_csv(os.path.join(input_dir, 'train_norm_luad_lscc.csv'))\n",
        "labels = {0:'normal', 1:'LUAD', 2:'LUSC'}\n",
        "fig2, axes = plt.subplots(1,6, figsize=(18,3))\n",
        "fig2.suptitle('Figure 2: Tiles from the training data')\n",
        "for i, idx in enumerate([125000*x for x in range(6)]): \n",
        "    axes[i].imshow(plt.imread(os.path.join(input_dir, training_data_list['path'][idx])))\n",
        "    axes[i].set_title(labels[training_data_list['reference_value'][idx]])\n",
        "    axes[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0q_tCWbODUk"
      },
      "source": [
        "# How many tiles per patient (due to different size of slides)?\n",
        "patient_metadata = pd.read_csv(os.path.join(input_dir, 'patient_metadata.csv'))\n",
        "print('Total number of tiles: {:,}'.format(patient_metadata['nr_tiles_total'].sum()))\n",
        "\n",
        "fig3 = sns.histplot(data=patient_metadata, x='nr_tiles_total', hue='cancer_subtype', multiple='stack', palette=['r', 'b'])\n",
        "fig3.set_title('Figure 3: Number of tiles per patient')\n",
        "fig3.set_xlabel('Number of tiles')\n",
        "fig3.set_ylabel('Number of patients')\n",
        "legend = fig3.get_legend()\n",
        "legend.set_title('Cancer subtype')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vXZO89ODUl"
      },
      "source": [
        "# Only for execution in VM --> remove later!\n",
        "# Create smaller training / validation and test sets (1/10 of the tiles)\n",
        "!sed -n '1p;0~40p' \"$input_dir/train_norm_luad_lscc.csv\" > \"$input_dir/train_norm_luad_lscc_40.csv\"\n",
        "!sed -n '1p;0~40p' \"$input_dir/valid_norm_luad_lscc.csv\" > \"$input_dir/valid_norm_luad_lscc_40.csv\"\n",
        "!sed -n '1p;0~40p' \"$input_dir/test_norm_luad_lscc.csv\" > \"$input_dir/test_norm_luad_lscc_40.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOQjGht5ODUl"
      },
      "source": [
        "### Training\n",
        "\n",
        "**Network training**: Following Coudray *et al.*, we use the InceptionV3 network architecture as implemented by Keras and shown in the figure below. The loss function is here defined as categorical cross entropy between the true labels and the network's predictions. RMSprop optimizer is utilized for learning with a learning rate of 0.1, weight decay of 0.9, momentum of 0.9 and epsilon of 1.0. Model checkpoints are automatically saved at each epoch in which the validation loss improves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbvT2mBXODUn"
      },
      "source": [
        "![inceptionV3.png](attachment:inceptionV3.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfRwefypODUn"
      },
      "source": [
        "from src.data.data_set import Dataset\n",
        "from src.model.inceptionv3_model import InceptionModel\n",
        "\n",
        "output_dir_experiment = os.path.join(output_dir, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
        "os.mkdir(output_dir_experiment)\n",
        "\n",
        "# Load datasets\n",
        "dataset_train = Dataset(os.path.join(input_dir, 'train_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "dataset_valid = Dataset(os.path.join(input_dir, 'valid_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "\n",
        "# Model set-up and training\n",
        "model = InceptionModel(num_classes=3, input_shape=(128,128,3), learning_rate=0.1)\n",
        "model.train(dataset_train, batch_size=512, epochs=10, output_path=output_dir_experiment, validation_dataset=dataset_valid) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sem3CkicODUo"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "The final model is selected as the one with the best (minimal) loss on the validation dataset. The performance of this model is evaluated quantitatively by means of receiver operating characteristic (ROC) curves and observed qualitatively using heatmap visualization. First, the model makes predictions for the entire test set and stores them in the experiment's output directory to make subsequent evaluation easily reproducible. \n",
        "\n",
        "**ROC analysis** for each of the three classes (normal, LUAD, LSCC) is performed on **tile-level** (tile-based ROC) and on **slide-level** (slide-based ROC). For slide-level analysis the probabilities for each slide were aggregated either by averaging the probabilities of the corresponding tiles, or by counting the percentage of tiles positively classified. Respective Area under the ROC curve (AUC) values are reported in the table below and the slide-based ROC curves are visualized in Figure 4. In addition to the ROC curves for each class, a micro-average ROC was computed and visualized. \n",
        "\n",
        "**Heatmaps** for visually inspecting the network's predictions are generated for some of the slides. The color of each tile corresponds to the class assigned by our model, with the hue indicating how confident the network is in this classification whereby a darker hue corresponds to higher confidence. The respective orginal WSI is shown as thumbnail image next to the heatmap. The generation of a thumbnail image takes ~1min, thus we limit ourselves to showing heatmaps for only 4 random slides from the testset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atrZ9kPbODUo"
      },
      "source": [
        "# Load the best model \n",
        "checkpoints = glob.glob(os.path.join(output_dir_experiment, 'checkpoint*/'))\n",
        "checkpoints.sort()\n",
        "best_model_dir = checkpoints[-1] # last checkpoint is the one with the best validation loss \n",
        "\n",
        "#from model.inceptionv3_model import InceptionModel\n",
        "#from data.data_set import Dataset\n",
        "#output_dir_experiment='/home/jupyter/idc_output/20210818_161953/'\n",
        "#best_model_dir = '/home/jupyter/idc_output/20210818_161953/checkpoint_008'\n",
        "\n",
        "best_model = InceptionModel.load(best_model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxm0hfOuODUp"
      },
      "source": [
        "from src.evaluation.predictions import Predictions\n",
        "\n",
        "test_dataset = Dataset(os.path.join(input_dir, 'test_norm_luad_lscc_40.csv'), num_classes=3)\n",
        "predictions = Predictions(best_model, test_dataset) \n",
        "predictions.save(os.path.join(output_dir_experiment, 'predictions_testset_40.json'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQBE9XSODUp"
      },
      "source": [
        "from src.evaluation.roc import ROCAnalysis\n",
        "\n",
        "roc_analysis = ROCAnalysis(predictions)\n",
        "roc_analysis.print_and_save_tabluar_results(os.path.join(output_dir_experiment, 'results_table_40.html'))\n",
        "roc_analysis.plot_and_save(output_dir_experiment) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aer0qDMhODUq"
      },
      "source": [
        "%%capture --no-display\n",
        "from src.data.utils import get_random_testset_slide_ids, get_thumbnail, get_slide_tissue_type\n",
        "from src.evaluation.heatmap import get_heatmap, plot_colormap_legend\n",
        "from src.evaluation.predictions import Predictions\n",
        "\n",
        "# Select slides and download thumbnails \n",
        "heatmap_slide_ids = get_random_testset_slide_ids(slides_metadata)\n",
        "#get_thumbnail(heatmap_slide_ids, slides_metadata_path, slides_dir, google_cloud_project_id=my_project_id) # NOTE: lasts ~1min per slide\n",
        "\n",
        "# Generate figure \n",
        "plot_colormap_legend()\n",
        "fig5, axes = plt.subplots(4, 2, figsize=(10, 9))\n",
        "fig5.suptitle('Figure 5: Heatmap visualization')\n",
        "for i in range(4):\n",
        "    slide_id = 'C3L-00913-22'#heatmap_slide_ids[i]\n",
        "    slide = plt.imread(os.path.join(slides_dir, slide_id + '.png'))\n",
        "    true_tissue_type = get_slide_tissue_type(slide_id, slides_metadata)\n",
        "    axes[i,0].imshow(get_heatmap(predictions, slide_id))\n",
        "    axes[i,1].imshow(slide1)\n",
        "    axes[i,1].set_title(true_tissue_type.upper())\n",
        "    axes[i,0].axis('off')\n",
        "    axes[i,1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RJclCq6Dw65"
      },
      "source": [
        "## Notes for improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU42L0ulODUr"
      },
      "source": [
        "### TODOs / Notes for myself \n",
        "- **Experienced in Colab that training progress is not printed and it takes quite long..what to do about that, if it appears more often?**\n",
        "    - AttributeError: 'NoneType' object has no attribute 'shape' --> documented in 03_pitfalls, maybe discuss with Andre\n",
        "- Check whether we have to change back model(patch_to_predict) to model.predict(patch_to_predict) when using TF/Keras 2.6.0\n",
        "- Check in general, that everything installed comes with a specific version\n",
        "- Add a comment about how long tile generation lasts for the user to know beforehand\n",
        "- Maybe verify successfull installations in notebook to catch errors there? \n",
        "- Get that \"Open in colab\" batch centered\n",
        "- sys.path.append('/usr/local/lib/python3.7/dist-packages') # otherwise Openslide cannot be loaded. Maybe find a better solution?!\n",
        "- Remove creation of smaller datasets from large files for everything on Google Colab \n",
        "- Heatmap add predicted class --> then we would have to decide on the threshold to use. \n",
        "- Adapt stepsize = len(training_data_list) // 10 when visualizing training tiles to make nice onces appear. \n",
        "- Remove try-except statement when opening slides --> we expect, that it works for all of them?!\n",
        "\n",
        "### What to polish\n",
        "--> Walk through all files to find futher stuff \n",
        "- Remove make_single_prediction?\n",
        "- Remove class_weights parameter completely from the code?\n",
        "- Check that TCGA-related stuff is removed or generalized e.g. in data_point \n",
        "- Make code nicer in roc.py when not showing confidence intervals \n",
        "\n",
        "### Final check\n",
        "- Move to IDC-Examples: adapt svn checkout, Github links in Environment setup text and in Colab Button\n",
        "- Check that all requirements are installed\n",
        "- Check that all warnings are suppressed/captured that we want to be captured\n",
        "- Adapt path to correct idc-Bucket and Slim-viewer\n",
        "- Check that everything which is only for execution in the VM / development purposes is removed.\n",
        "- Walk through explanatory text and ensure everything is correct! \n",
        "    - Generation of every x-th tile: Check that x is set correctly and matches what stands in the text\n",
        "    - Adapt text about how many slides there are etc. \n",
        "- Make sure, that notebook is saved without output or rather with correct and nice looking output \n",
        "- Check if all images (e.g. InceptionV3 or plotted figures) are displayed nicely in Colab\n",
        "\n",
        "### Other notes: \n",
        "- claim GPU only when we are using it, i.e. for training! --> GPU has to be included at the very beginning, other stuff stored is removed otherwise (even Google authentication and variables) \n",
        "\n",
        "## To run notebook on VM\n",
        "- remove Google Colab authentification\n",
        "- remove GPU activation\n",
        "- remove Github/svn checkout\n",
        "- change input/output directory paths \n",
        "- Add again creation of smaller datasets from large files (and set save_every_xth_tile=1, if tiles not already downloaded)\n",
        "\n",
        "## Open questions\n",
        "- Why do we have so much more tiles than with TCGA data? \n",
        "\n",
        "### Answered:\n",
        "- When will CPTAC data go to official IDC and in which bucket?\n",
        "    - very soon and into the idc-open bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rorARTX4ODUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}