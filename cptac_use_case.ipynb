{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWemgwQEODUQ"
   },
   "outputs": [],
   "source": [
    "# only for development, can be removed later\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XZJ4whKODUM"
   },
   "source": [
    "# IDC Tutorial: Tissue classification in slide microscopy images\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ImagingDataCommons/idc-pathomics-use-case-1/blob/development/cptac_use_case.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use the [Imaging Data Commons (IDC)](https://portal.imaging.datacommons.cancer.gov/) for pathomics research:\n",
    "\n",
    "* how to select, access, and download cohorts from the IDC;\n",
    "* how to access and process slide microscopy images in DICOM format;\n",
    "* how to run an end-to-end analysis pipeline and visualize the final results exploiting the IDC platform.\n",
    "\n",
    "Pathomics refers to the computerized analysis of slide microscopy images. Besides radiology, slide microscopy is the second major imaging modality in the IDC. Slide microscopy images show thin sections of tissue samples (e.g., from a resected tumor) at microscopic resolution. They provide a unique glimpse into cellular architecture and function that is essential for diagnosing complex diseases like cancer. Computerized analysis makes the assessment of slide microscopy images more reproducible and less time consuming and it enables the extration of novel digital biomarkers from tissue images.\n",
    "\n",
    "This tutorial builds on the publication \"Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning\" ([Coudray et al. 2018](https://doi.org/10.1038/s41591-018-0177-5)), one of the most cited pathomics publications in recent years. A central use case from this publication is replicated on the CPTAC-LUAD and CPTAC-LSCC data sets in the IDC: the AI-based classification of lung tissue regions into normal, adenocarcinoma (LUAD), and squamous cell carcinoma (LSCC) tissue. Your own tissue classification use cases can be solved in a similar manner.\n",
    "  \n",
    "To be quickly and freely accessible to everyone, this tutorial was deliberately kept simple and designed to be run in Google Colab. It highlights only a small part of what the IDC can offer in terms of data exploration and imaging analysis. More complex use cases can be implemented using GCP virtual machines. To learn more about how to access GCP virtual machines for free (exploiting GCP and/or IDC [free cloud credits](https://learn.canceridc.dev/introduction/requesting-gcp-cloud-credits)) and about the IDC platform, please visit the [IDC user guide](https://learn.canceridc.dev/).\n",
    "\n",
    "If you have any questions, bug reports, or feature requests please feel free to contact us at the [IDC discussion forum](https://discourse.canceridc.dev/)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyITRKQCGpWY"
   },
   "source": [
    "## Customization\n",
    "\n",
    "Before we begin, some individual adjustments must be made as described in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD4pg2s1ODUT"
   },
   "source": [
    "This notebook is intended to be run using a **GPU**. In Google Colab, GPU usage can be enabled via `Edit > Notebooks Settings > Hardware accelerator`. Since Collab notebooks are assigned arbitrary GPUs (eg., NVIDIA K80, T4, P4 and P100), processing times can vary. The code below checks whether GPU usage has been enabled and which GPU type the Colab instance is equipped with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik2dHbpXODUU"
   },
   "outputs": [],
   "source": [
    "gpu_list = !nvidia-smi --list-gpus\n",
    "has_gpu = False if 'failed' in gpu_list[0] else True\n",
    "print(has_gpu, gpu_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEBmNHEmDUlU"
   },
   "source": [
    "To access IDC resources, you have to authenticate with your **Google identity**. Follow the link generated by the code below and enter the displayed verification code to complete the Google authentication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EbKWd-q1ODUW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRG25ZsVdOv-"
   },
   "source": [
    "Access to IDC resources must happen in the context of a **Google Cloud Platform project**. You can learn how to create your own project [here](https://www.youtube.com/watch?v=i08S0KJLnyw). Set `my_project_id` below to the ID of your GCP project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHV_UoHAODUX"
   },
   "outputs": [],
   "source": [
    "my_project_id = 'idc-pathomics-000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OgrV_UTag7n"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJWt5C8CCNRU"
   },
   "source": [
    "This notebook relies on utility code for preprocessing, model training and evaluation, which is provided in an open-source Github repository. In Google Colab, the easiest way to retrieve subdirectories from GitHub is with the Apache Subversion tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1kAyZnZODUa"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y subversion \n",
    "!svn checkout https://github.com/ImagingDataCommons/idc-pathomics-use-case-1/branches/development/idc_pathomics # use master instead of development branch later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE2PnoKQCd1K"
   },
   "source": [
    "Most of the Python packages required by this notebook are pre-installed in a Google Colab instance. In addition, we need to install the [OpenSlide](https://openslide.org/api/python/) library for accessing whole-slide images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXn9b0BfODUb"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install --no-install-recommends -y python3-openslide\n",
    "!sudo pip3 install openslide-wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPHDqvzCCzzj"
   },
   "source": [
    "Import the required Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWDMlEUXODUc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.7/dist-packages') # otherwise Openslide cannot be loaded.\n",
    "import glob \n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import warnings \n",
    "with warnings.catch_warnings(): # Hide python warnings to improve readability.\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccy1m_azC8yh"
   },
   "source": [
    "Determine who and where we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENz1CUE3ODUY"
   },
   "outputs": [],
   "source": [
    "curr_dir = !pwd\n",
    "curr_droid = !hostname\n",
    "curr_pilot = !whoami\n",
    "\n",
    "print('Current directory :', curr_dir[-1])\n",
    "print('Hostname          :', curr_droid[-1])\n",
    "print('Username          :', curr_pilot[-1])\n",
    "!rm -rf sample_data # remove /content/sample_data directory which is included in a Google Colab instance by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbKvSMirNKJa"
   },
   "source": [
    "Create directories for input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFuEQ3tXNPDy"
   },
   "outputs": [],
   "source": [
    "input_dir = '/content/idc_input/'\n",
    "output_dir = '/content/idc_output'\n",
    "# Use the following paths if using a Google Cloud VM instead of Google Colab\n",
    "#input_dir = '/home/jupyter/idc_input/'\n",
    "#output_dir = '/home/jupyter/idc_output/'\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGvZhkpyODUc"
   },
   "source": [
    "## Dataset selection and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM1g60Hx_Iij"
   },
   "source": [
    "IDC relies on the Google Cloud Platform (GCP) for storage and management of DICOM data. The data are contained in so-called [storage buckets](https://cloud.google.com/storage/docs/key-terms#buckets), from which they can be retrieved on a requester pays basis. Currently, all pathology whole-slide images (WSI) are located in the `idc-open` bucket.\n",
    "\n",
    "Metadata for the DICOM files—including standard DICOM tags, but also non-DICOM metadata—are stored in the BigQuery table `dicom_all`. The IDC Documentation gives further information on [data organization](https://learn.canceridc.dev/data/organization-of-data) and [code examples](https://learn.canceridc.dev/cookbook/bigquery) on how to query the table. The easiest way to access BigQuery tables from a Jupyter notebook is to use [BigQuery cell magic](https://cloud.google.com/bigquery/docs/visualize-jupyter#querying-and-visualizing-bigquery-data) using the `%%bigquery` command. \n",
    "\n",
    "The following statement loads relevant metadata of all slide images from the CPTAC-LUAD and CPTAC-LSCC datasets into a pandas data frame called `slides_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSIKW0VJODUe"
   },
   "outputs": [],
   "source": [
    "%%bigquery slides_df --project=$my_project_id \n",
    "\n",
    "SELECT\n",
    "    ContainerIdentifier AS slide_id,\n",
    "    PatientID AS patient_id,\n",
    "    ClinicalTrialProtocolID AS dataset,\n",
    "    TotalPixelMatrixColumns AS width,\n",
    "    TotalPixelMatrixRows AS height,\n",
    "    StudyInstanceUID AS idc_viewer_id,        \n",
    "    gcs_url, -- URL of the Google Cloud storage bucket\n",
    "    CAST(SharedFunctionalGroupsSequence[OFFSET(0)].\n",
    "          PixelMeasuresSequence[OFFSET(0)].\n",
    "          PixelSpacing[OFFSET(0)] AS FLOAT64) AS pixel_spacing,\n",
    "    -- rename TransferSyntaxUIDs for readability\n",
    "    CASE TransferSyntaxUID\n",
    "        WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'\n",
    "        WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'\n",
    "        ELSE 'other'\n",
    "    END AS compression\n",
    "FROM idc-dev-etl.idc_v3.dicom_all\n",
    "WHERE\n",
    "  NOT (ContainerIdentifier IS NULL)\n",
    "  AND (ClinicalTrialProtocolID = \"CPTAC-LUAD\" OR ClinicalTrialProtocolID = \"CPTAC-LSCC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk8XYRkhAHu8"
   },
   "source": [
    "We reduce the obtained data frame to the images we actually want to use for the analysis. Those are required to be digitized at 20x magnification (corresponding to a pixel spacing between 0.00025 and 0.00051 mm) and compressed in JPEG format (to be readable by the OpenSlide libray). Also, we consider only every 5th slide to make the dataset small enough to be manageable in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGhIxoWh5bU0"
   },
   "outputs": [],
   "source": [
    "slides_df.query('pixel_spacing > 0.00025 & pixel_spacing < 0.00051 & compression==\"jpeg\"', inplace=True)\n",
    "slides_df = slides_df.iloc[::5, :] # select every 5th row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUDmRLQaODUf"
   },
   "source": [
    "The tissue type of the slides (tumor or normal) is not yet included in the `dicom_all` table. This information has to be supplemented from a separate CSV file provided by the TCIA. The cancer subtype (LSCC or LUAD) can in principle be inferred from the dataset name, but for clarity we also use the information in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g84bWWjBODUg"
   },
   "outputs": [],
   "source": [
    "type_df = pd.read_csv('idc_pathomics/tissue_type_data_TCIA.csv')[['Slide_ID', 'Specimen_Type', 'Tumor']]\n",
    "# harmonize column names and labels\n",
    "type_df.rename(columns={'Slide_ID': 'slide_id', 'Specimen_Type': 'tissue_type', 'Tumor': 'cancer_subtype'}, inplace=True)\n",
    "type_df.replace({'tissue_type': {'normal_tissue': 'normal', 'tumor_tissue': 'tumor'}}, inplace=True)\n",
    "type_df.replace({'cancer_subtype': {'LSCC': 'lscc', 'LUAD': 'luad'}}, inplace=True)\n",
    "slides_df = pd.merge(slides_df, type_df, how='inner', on='slide_id', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CqUnbMUODUg"
   },
   "source": [
    "With standard [pandas](https://pandas.pydata.org/) functionality, we can easily validate and summarize the compiled metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IhrfoqyODUh"
   },
   "outputs": [],
   "source": [
    "# assert uniqueness of slide_id values\n",
    "assert(slides_df.slide_id.is_unique)\n",
    "\n",
    "# assert validity of class labels\n",
    "assert set(slides_df.tissue_type.unique()) == set(['normal', 'tumor'])\n",
    "assert set(slides_df.cancer_subtype.unique()) == set(['luad', 'lscc'])\n",
    "\n",
    "display(slides_df.head())\n",
    "print('Total number of slides: ', len(slides_df))\n",
    "nr_slides = slides_df.groupby('cancer_subtype').size()\n",
    "nr_patients = slides_df.drop_duplicates('patient_id').groupby('cancer_subtype').size()\n",
    "print('--> %d slides from %d LUAD patients' % (nr_slides['luad'], nr_patients['luad']))\n",
    "print('--> %d slides from %d LSCC patients' % (nr_slides['lscc'], nr_patients['lscc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnDVVjlPODUh"
   },
   "source": [
    "Using standard [matplotlib](https://matplotlib.org/) functionality, we can easily visualize some aspects of interest. The following code produces two histograms. The left graph shows the frequencies of numbers of slides per per patient, while the right graph shows the proportions of slides derived from healthy or tumor tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZR93DngODUh"
   },
   "outputs": [],
   "source": [
    "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# number of slides per patient \n",
    "slides_per_patient = slides_df.groupby(['patient_id']).size()\n",
    "plot1 = sns.histplot(data=slides_per_patient, discrete=True, ax=ax1, shrink=0.9, color=['C7'])\n",
    "ax1.update({'xlabel': 'Number of slides', 'ylabel': 'Number of patients'})\n",
    "\n",
    "# distribution of tissue types\n",
    "plot2 = sns.histplot(data=slides_df, x='cancer_subtype', hue='tissue_type', multiple='stack', palette = ['C1', 'C2'], ax=ax2, shrink=0.7)\n",
    "ax2.update({'xlabel': 'Cancer subtype', 'ylabel': 'Number of slides'})\n",
    "legend = plot2.get_legend()\n",
    "legend.set_title('Tissue type')\n",
    "legend.set_bbox_to_anchor((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM5lb-qLODUh"
   },
   "source": [
    "Any slide can also be easily viewed and explored in detail using the IDC viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_eQc54DODUi"
   },
   "outputs": [],
   "source": [
    "def get_idc_viewer_url(study_UID):\n",
    "    return \"https://viewer.imaging.datacommons.cancer.gov/slim/studies/\" + study_UID\n",
    "\n",
    "print(get_idc_viewer_url(slides_df['idc_viewer_id'].iloc[0]))\n",
    "print(get_idc_viewer_url(slides_df['idc_viewer_id'].iloc[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRA-t7_v8Yv3"
   },
   "source": [
    "Finally, we save the information as CSV file `slides_metadata.csv` to be used later for splitting into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YF59Er-b8itA"
   },
   "outputs": [],
   "source": [
    "slides_metadata_path = os.path.join(input_dir, 'slides_metadata.csv')\n",
    "slides_df.to_csv(slides_metadata_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SS01aq3DnvS"
   },
   "source": [
    "## Pathomics experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgjuwjmXODUi"
   },
   "source": [
    "The following pathomics experiment consists of three main steps:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "    - Tiling of slides and filtering out of background tiles\n",
    "    - Separation into training, validation and test dataset\n",
    "2. **Training**: Per-tile training of the model\n",
    "3. **Evaluation**: Per-tile and per-slide evaluation of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbOrYn_PODUi"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "**Tile generation**: First, the WSI are downloaded with `gsutil` and the respective \"gcs_url\"s. Then each slide is tiled into non-overlapping 128x128 pixel windows at a magnification of 5x using the python openslide library. As this code is supposed to serve as a demo requiring a minimum amount of runtime, we reduce each datasets size by generating only one in `twenty` tiles. \n",
    "\n",
    "**Generation of datasets**: Next, the tiles are distributed among / sorted into training (70%), test (15%) and validation (15%) datasets, ensuring that tiles associated with a particular patient are not separated, but assigned as a whole to one of these sets. For this purpose, a file (*patient_metadata.csv*) is created that reports the number of tiles created for each patient and is used to generate Figure 3 below. \\\n",
    "Additionally, we update *slides_metadata.csv* by adding the information whether a slide has been assigned to the training, validation or testset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sqJ3DDBODUj"
   },
   "outputs": [],
   "source": [
    "from idc_pathomics.data.tile_generation_cptac import generate_tiles\n",
    "\n",
    "slides_dir = os.path.join(input_dir, 'cptac_slides')\n",
    "tiles_dir = os.path.join(input_dir, 'cptac_tiles')\n",
    "if not os.path.exists(slides_dir):\n",
    "    os.makedirs(slides_dir)\n",
    "\n",
    "if not os.path.exists(tiles_dir):\n",
    "    os.makedirs(tiles_dir)\n",
    "\n",
    "#generate_tiles(slides_dir, slides_metadata_path, tiles_dir, save_every_xth_tile=20, google_cloud_project_id=my_project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRHbIeWSODUj"
   },
   "outputs": [],
   "source": [
    "from idc_pathomics.data.tile_sorting_cptac import sort_tiles\n",
    "\n",
    "sort_tiles(tiles_dir, slides_metadata_path, output_folder=input_dir)\n",
    "slides_metadata = pd.read_csv(slides_metadata_path) # reload updated slides_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftBwYUG8ODUj"
   },
   "outputs": [],
   "source": [
    "# Visualize some tiles from the training dataset\n",
    "training_data_list = pd.read_csv(os.path.join(input_dir, 'train_norm_luad_lscc.csv'))\n",
    "labels = {0:'normal', 1:'LUAD', 2:'LUSC'}\n",
    "fig2, axes = plt.subplots(1,6, figsize=(18,3))\n",
    "fig2.suptitle('Figure 2: Tiles from the training data')\n",
    "for i, idx in enumerate([125000*x for x in range(6)]): \n",
    "    axes[i].imshow(plt.imread(os.path.join(input_dir, training_data_list['path'][idx])))\n",
    "    axes[i].set_title(labels[training_data_list['reference_value'][idx]])\n",
    "    axes[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0q_tCWbODUk"
   },
   "outputs": [],
   "source": [
    "# How many tiles per patient (due to different size of slides)?\n",
    "patient_metadata = pd.read_csv(os.path.join(input_dir, 'patient_metadata.csv'))\n",
    "print('Total number of tiles: {:,}'.format(patient_metadata['nr_tiles_total'].sum()))\n",
    "\n",
    "fig3 = sns.histplot(data=patient_metadata, x='nr_tiles_total', hue='cancer_subtype', multiple='stack', palette=['r', 'b'])\n",
    "fig3.set_title('Figure 3: Number of tiles per patient')\n",
    "fig3.update({'xlabel': 'Number of tiles', 'ylabel': 'Number of patients'})\n",
    "legend = fig3.get_legend()\n",
    "legend.set_title('Cancer subtype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6vXZO89ODUl"
   },
   "outputs": [],
   "source": [
    "# Only for execution in VM --> remove later!\n",
    "# Create smaller training / validation and test sets (1/10 of the tiles)\n",
    "!sed -n '1p;0~40p' \"$input_dir/train_norm_luad_lscc.csv\" > \"$input_dir/train_norm_luad_lscc_40.csv\"\n",
    "!sed -n '1p;0~40p' \"$input_dir/valid_norm_luad_lscc.csv\" > \"$input_dir/valid_norm_luad_lscc_40.csv\"\n",
    "!sed -n '1p;0~40p' \"$input_dir/test_norm_luad_lscc.csv\" > \"$input_dir/test_norm_luad_lscc_40.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOQjGht5ODUl"
   },
   "source": [
    "### Training\n",
    "\n",
    "**Network training**: Following Coudray *et al.*, we use the InceptionV3 network architecture as implemented by Keras and shown in the figure below. The loss function is here defined as categorical cross entropy between the true labels and the network's predictions. RMSprop optimizer is utilized for learning with a learning rate of 0.1, weight decay of 0.9, momentum of 0.9 and epsilon of 1.0. Model checkpoints are automatically saved at each epoch in which the validation loss improves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbvT2mBXODUn"
   },
   "source": [
    "![inceptionV3.png](attachment:inceptionV3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfRwefypODUn"
   },
   "outputs": [],
   "source": [
    "from idc_pathomics.data.data_set import Dataset\n",
    "from idc_pathomics.model.inceptionv3_model import InceptionModel\n",
    "\n",
    "output_dir_experiment = os.path.join(output_dir, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.mkdir(output_dir_experiment)\n",
    "\n",
    "# Load datasets\n",
    "dataset_train = Dataset(os.path.join(input_dir, 'train_norm_luad_lscc_40.csv'), num_classes=3)\n",
    "dataset_valid = Dataset(os.path.join(input_dir, 'valid_norm_luad_lscc_40.csv'), num_classes=3)\n",
    "\n",
    "# Model set-up and training\n",
    "model = InceptionModel(num_classes=3, input_shape=(128,128,3), learning_rate=0.1)\n",
    "model.train(dataset_train, batch_size=512, epochs=10, output_path=output_dir_experiment, validation_dataset=dataset_valid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sem3CkicODUo"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The final model is selected as the one with the best (minimal) loss on the validation dataset. The performance of this model is evaluated quantitatively by means of receiver operating characteristic (ROC) curves and observed qualitatively using heatmap visualization. First, the model makes predictions for the entire test set and stores them in the experiment's output directory to make subsequent evaluation easily reproducible. \n",
    "\n",
    "**ROC analysis** for each of the three classes (normal, LUAD, LSCC) is performed on **tile-level** (tile-based ROC) and on **slide-level** (slide-based ROC). For slide-level analysis the probabilities for each slide were aggregated either by averaging the probabilities of the corresponding tiles, or by counting the percentage of tiles positively classified. Respective Area under the ROC curve (AUC) values are reported in the table below and the slide-based ROC curves are visualized in Figure 4. In addition to the ROC curves for each class, a micro-average ROC was computed and visualized. \n",
    "\n",
    "**Heatmaps** for visually inspecting the network's predictions are generated for some of the slides. The color of each tile corresponds to the class assigned by our model, with the hue indicating how confident the network is in this classification whereby a darker hue corresponds to higher confidence. The respective orginal WSI is shown as thumbnail image next to the heatmap. The generation of a thumbnail image takes ~1min, thus we limit ourselves to showing heatmaps for only 4 random slides from the testset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atrZ9kPbODUo"
   },
   "outputs": [],
   "source": [
    "# Load the best model \n",
    "checkpoints = glob.glob(os.path.join(output_dir_experiment, 'checkpoint*/'))\n",
    "checkpoints.sort()\n",
    "best_model_dir = checkpoints[-1] # last checkpoint is the one with the best validation loss \n",
    "\n",
    "#from model.inceptionv3_model import InceptionModel\n",
    "#from data.data_set import Dataset\n",
    "#output_dir_experiment='/home/jupyter/idc_output/20210818_161953/'\n",
    "#best_model_dir = '/home/jupyter/idc_output/20210818_161953/checkpoint_008'\n",
    "\n",
    "best_model = InceptionModel.load(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxm0hfOuODUp"
   },
   "outputs": [],
   "source": [
    "from idc_pathomics.evaluation.predictions import Predictions\n",
    "\n",
    "test_dataset = Dataset(os.path.join(input_dir, 'test_norm_luad_lscc_40.csv'), num_classes=3)\n",
    "predictions = Predictions(best_model, test_dataset) \n",
    "predictions.save(os.path.join(output_dir_experiment, 'predictions_testset_40.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kQBE9XSODUp"
   },
   "outputs": [],
   "source": [
    "from idc_pathomics.evaluation.roc import ROCAnalysis\n",
    "\n",
    "roc_analysis = ROCAnalysis(predictions)\n",
    "roc_analysis.print_and_save_tabluar_results(os.path.join(output_dir_experiment, 'results_table_40.html'))\n",
    "roc_analysis.plot_and_save(output_dir_experiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aer0qDMhODUq"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from idc_pathomics.data.utils import get_random_testset_slide_ids, get_thumbnail, get_slide_tissue_type\n",
    "from idc_pathomics.evaluation.heatmap import get_heatmap, plot_colormap_legend\n",
    "from idc_pathomics.evaluation.predictions import Predictions\n",
    "\n",
    "# Select slides and download thumbnails \n",
    "heatmap_slide_ids = get_random_testset_slide_ids(slides_metadata)\n",
    "#get_thumbnail(heatmap_slide_ids, slides_metadata_path, slides_dir, google_cloud_project_id=my_project_id) # NOTE: lasts ~1min per slide\n",
    "\n",
    "# Generate figure \n",
    "plot_colormap_legend()\n",
    "fig5, axes = plt.subplots(4, 2, figsize=(10, 9))\n",
    "fig5.suptitle('Figure 5: Heatmap visualization')\n",
    "for i in range(4):\n",
    "    slide_id = 'C3L-00913-22'#heatmap_slide_ids[i]\n",
    "    slide = plt.imread(os.path.join(slides_dir, slide_id + '.png'))\n",
    "    true_tissue_type = get_slide_tissue_type(slide_id, slides_metadata)\n",
    "    axes[i,0].imshow(get_heatmap(predictions, slide_id))\n",
    "    axes[i,1].imshow(slide1)\n",
    "    axes[i,1].set_title(true_tissue_type.upper())\n",
    "    axes[i,0].axis('off')\n",
    "    axes[i,1].axis('off')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cptac_use_case.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
